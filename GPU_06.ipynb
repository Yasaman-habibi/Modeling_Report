{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO37+Utff8UOzA/efu8l2SO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yasaman-habibi/Modeling_Report/blob/main/GPU_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWUgzPEulSt-"
      },
      "outputs": [],
      "source": [
        "# config.py\n",
        "\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "class Config:\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # ۱. مسیرها (سازگار با Linux / GPU Server)\n",
        "    # -----------------------------------------------------\n",
        "    BASE_DIR = Path(\".\").resolve()\n",
        "\n",
        "    INPUT_DIR = BASE_DIR / \"data\" / \"input\"\n",
        "    OUTPUT_DIR = BASE_DIR / \"data\" / \"output\"\n",
        "\n",
        "    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # ۲. کلاس‌ها\n",
        "    # -----------------------------------------------------\n",
        "    CLASSES = [\"E\", \"S\", \"G\" , \"EC\"]\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # ۳. Embedding / GPU\n",
        "    # -----------------------------------------------------\n",
        "    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    USE_FP16 = True if DEVICE == \"cuda\" else False\n",
        "\n",
        "    BATCH_SIZE_GPU = 256\n",
        "    BATCH_SIZE_CPU = 32\n",
        "\n",
        "    BATCH_SIZE = BATCH_SIZE_GPU if DEVICE == \"cuda\" else BATCH_SIZE_CPU\n",
        "\n",
        "    -------------------------------------------------------\n",
        "        CLASS_SEEDS = {\n",
        "        \"E\": [\n",
        "            \"emission\", \"carbon\", \"Environmental\", \"Environmental protection\", \"Environmental impact\",\n",
        "            \"environment\", \"climate\", \"Climate mitigation\", \"Climate change\", \"Carbon footprint\",\n",
        "            \"Carbon emissions\", \"Pollutants\", \"Greenhouse Gas Emissions\", \"Decarbonization\",\n",
        "            \"Renewable energy\", \"Clean energy\", \"Energy efficiency\", \"Recycling\", \"Circular economy\",\n",
        "            \"Waste management\", \"Zero waste\", \"Natural Resources\", \"Resource management\",\n",
        "            \"Earth\", \"Air\", \"biodiversity\", \"Atmospheric\", \"Water\", \"pollution\",  \"Green technology\",\n",
        "            \"Pollution reduction\", \"Drought\", \"Water conservation\", \"Ground Warming\",\n",
        "            \"Global Warming\", \"Species extinction\", \"Ecosystem preservation\", \"Sustainable materials\",\n",
        "            \"Life cycle assessment\", \"Eco-friendly\", \"Sustainable agriculture\",\n",
        "        ],\n",
        "        \"S\": [\n",
        "            \"employee\", \"safety\", \"Human rights\", \"CSR\", \"Corporate\", \"social\", \"responsibility\", \"Society\",\n",
        "            \"Responsible consumption\", \"Demographic changes\", \"Famine\", \"Better life\", \"Diversity\", \"Inclusion\",\n",
        "            \"Equality\", \"Labor practices\", \"Worker health\", \"Sustainable supply chain\",\n",
        "        ],\n",
        "        \"G\": [\n",
        "            \"Economic\", \"Economy\", \"Green economy\", \"governance\", \"Risk management\", \"Executive compensation\",\n",
        "            \"Shareholder\", \"audit\", \"board\", \"Management structure\", \"Fiduciary duty\", \"Internal controls\",\n",
        "            \"Ethics\", \"Compliance\", \"Regulation\", \"Anti-corruption\", \"Corruption\", \"Bribery\", \"Legal\",\n",
        "            \"Code of conduct\", \"Disclosure\", \"Transparency\", \"Non-Financial\", \"Reporting\", \"Accountability\",\n",
        "            \"Data security\",\n",
        "        ],\n",
        "        \"EC\": [\n",
        "            \"economic\", \"financial performance\", \"profitability\",\n",
        "            \"economic growth\", \"market risk\", \"capital allocation\",\n",
        "            \"investment\", \"revenue\", \"cost efficiency\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # ۴. Seed\n",
        "    # -----------------------------------------------------\n",
        "    RANDOM_STATE = 42\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # ۵. وزن امتیازها\n",
        "    # -----------------------------------------------------\n",
        "    ALPHA = 0.5\n",
        "    GAMMA = 0.3\n",
        "    BETA = 0.2\n",
        "\n",
        "    if not abs(ALPHA + GAMMA + BETA - 1.0) < 1e-6:\n",
        "        print(\"⚠️ هشدار: مجموع وزن‌ها برابر ۱ نیست\")\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # ۶. آستانه‌های تصمیم\n",
        "    # -----------------------------------------------------\n",
        "    THRESH_LABEL = 0.6\n",
        "    THRESH_MULTI = 0.45\n",
        "    THRESH_NONE  = 0.4\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # ۷. Chunking\n",
        "    # -----------------------------------------------------\n",
        "    MAX_CHUNK_TOKENS = 512\n",
        "    SPLIT_SIZE = 256\n",
        "    SPLIT_OVERLAP = 50\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # ۸. Lexicon (TF-IDF)\n",
        "    # -----------------------------------------------------\n",
        "    TFIDF_TOPN = 50\n",
        "    TFIDF_MIN_DF = 2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# io_utils.py\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Iterator, Dict, Any\n",
        "\n",
        "try:\n",
        "    from .config import Config\n",
        "except ImportError:\n",
        "    class Config:\n",
        "        MIN_CHUNK_CHARS = 10\n",
        "        CLASSES = [\"E\", \"S\", \"G\" , \"EC\"]\n",
        "\n",
        "\n",
        "def iter_cleaned_chunks(input_dir: Path) -> Iterator[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Generator برای خواندن چانک‌ها بدون نگه داشتن کل داده در RAM\n",
        "    \"\"\"\n",
        "    json_files = list(input_dir.glob(\"*.json\"))\n",
        "    print(f\"در حال پردازش {len(json_files)} فایل JSON...\")\n",
        "\n",
        "    for file_path in tqdm(json_files, desc=\"Processing JSON Files\"):\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            document_id = file_path.stem\n",
        "\n",
        "            for key, chunk_data in data.items():\n",
        "                if key.startswith(\"chunk_\") and isinstance(chunk_data, dict):\n",
        "                    text = chunk_data.get(\"chunk_text\", \"\").strip()\n",
        "\n",
        "                    if text and len(text) >= Config.MIN_CHUNK_CHARS:\n",
        "                        yield {\n",
        "                            \"chunk_id\": chunk_data.get(\"chunk_id\"),\n",
        "                            \"document_id\": document_id,\n",
        "                            \"chunk_text\": text,\n",
        "                            \"item_code\": chunk_data.get(\"item_code\", \"N/A\"),\n",
        "                            \"E_score\": 0.0,\n",
        "                            \"S_score\": 0.0,\n",
        "                            \"G_score\": 0.0,\n",
        "                            \"EC_score\": 0.0,\n",
        "                            \"final_label\": \"None\",\n",
        "                        }\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"⚠️ فایل {file_path} JSON معتبر نیست.\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ خطا در {file_path}: {e}\")\n",
        "\n",
        "\n",
        "def load_cleaned_jsons(input_dir: Path, chunk_size: int = 50_000) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    بارگذاری چانک‌ها به صورت batch برای جلوگیری از مصرف بیش از حد RAM\n",
        "    \"\"\"\n",
        "    buffer = []\n",
        "    frames = []\n",
        "\n",
        "    for row in iter_cleaned_chunks(input_dir):\n",
        "        buffer.append(row)\n",
        "\n",
        "        if len(buffer) >= chunk_size:\n",
        "            df_part = pd.DataFrame(buffer)\n",
        "            frames.append(df_part)\n",
        "            buffer.clear()\n",
        "\n",
        "    if buffer:\n",
        "        frames.append(pd.DataFrame(buffer))\n",
        "\n",
        "    if not frames:\n",
        "        print(\"هشدار: هیچ چانک معتبری پیدا نشد.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    if \"chunk_id\" in df.columns:\n",
        "        df.set_index(\"chunk_id\", inplace=True)\n",
        "\n",
        "    print(f\"\\nتعداد کل چانک‌های استخراج شده: {len(df)}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def save_scored_dataframe(df: pd.DataFrame, output_dir: Path, filename: str = \"scored_chunks.csv\"):\n",
        "    output_path = output_dir / filename\n",
        "    df.to_csv(output_path, index=True, encoding=\"utf-8\")\n",
        "    print(f\"\\nDataFrame در {output_path} ذخیره شد.\")\n",
        "\n",
        "\n",
        "def load_previous_scores(output_dir: Path, filename: str = \"scored_chunks.csv\") -> pd.DataFrame:\n",
        "    try:\n",
        "        df = pd.read_csv(output_dir / filename, index_col=\"chunk_id\", encoding=\"utf-8\")\n",
        "        print(f\"DataFrame قبلی با {len(df)} سطر بارگذاری شد.\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(\"DataFrame قبلی پیدا نشد.\")\n",
        "        return pd.DataFrame()\n"
      ],
      "metadata": {
        "id": "ghbiEFRaluvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing.py\n",
        "\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "try:\n",
        "    from .config import Config\n",
        "except ImportError:\n",
        "    class Config:\n",
        "        MIN_CHUNK_CHARS = 10\n",
        "        MAX_CHUNK_TOKENS = 512\n",
        "        SPLIT_OVERLAP = 50\n",
        "        EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
        "        INPUT_DIR = Path(\"./raw_data\")\n",
        "        OUTPUT_DIR = Path(\"./results\")\n",
        "\n",
        "\n",
        "# --------------------------------\n",
        "# 1. پاک‌سازی متن\n",
        "# --------------------------------\n",
        "def clean_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'-{3,}', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "# --------------------------------\n",
        "# 2. Chunking واقعی بر اساس tokenizer مدل\n",
        "# --------------------------------\n",
        "def split_text_into_chunks_by_tokens(\n",
        "    text: str,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    max_tokens: int,\n",
        "    overlap: int\n",
        ") -> List[Dict[str, Any]]:\n",
        "\n",
        "    input_ids = tokenizer.encode(\n",
        "        text,\n",
        "        add_special_tokens=False,\n",
        "        truncation=False\n",
        "    )\n",
        "\n",
        "    chunks = []\n",
        "    step = max_tokens - overlap\n",
        "    if step <= 0:\n",
        "        raise ValueError(\"overlap must be smaller than max_tokens\")\n",
        "\n",
        "    chunk_index = 0\n",
        "    for i in range(0, len(input_ids), step):\n",
        "        chunk_ids = input_ids[i:i + max_tokens]\n",
        "        chunk_text = tokenizer.decode(chunk_ids)\n",
        "\n",
        "        if len(chunk_text) >= Config.MIN_CHUNK_CHARS:\n",
        "            chunks.append({\n",
        "                \"chunk_text\": chunk_text,\n",
        "                \"n_tokens\": len(chunk_ids),\n",
        "                \"chunk_index\": chunk_index\n",
        "            })\n",
        "            chunk_index += 1\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# --------------------------------\n",
        "# 3. اجرای خط لوله پیش‌پردازش\n",
        "# --------------------------------\n",
        "def run_preprocessing_pipeline(\n",
        "    input_dir: Path,\n",
        "    output_dir: Path,\n",
        "    config: Any\n",
        "):\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.EMBEDDING_MODEL)\n",
        "\n",
        "    raw_files = list(input_dir.glob(\"*.txt\"))\n",
        "    print(f\"شروع پیش‌پردازش برای {len(raw_files)} فایل متنی\")\n",
        "\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for file_path in tqdm(raw_files, desc=\"Preprocessing\"):\n",
        "        document_id = file_path.stem\n",
        "        output_json_path = output_dir / f\"cleaned_{document_id}.json\"\n",
        "\n",
        "        if output_json_path.exists():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                raw_text = f.read()\n",
        "\n",
        "            cleaned_text = clean_text(raw_text)\n",
        "\n",
        "            chunks = split_text_into_chunks_by_tokens(\n",
        "                cleaned_text,\n",
        "                tokenizer,\n",
        "                config.MAX_CHUNK_TOKENS,\n",
        "                config.SPLIT_OVERLAP\n",
        "            )\n",
        "\n",
        "            output_data = {}\n",
        "            for idx, chunk in enumerate(chunks):\n",
        "                chunk_key = f\"chunk_{idx:03d}\"\n",
        "                unique_chunk_id = f\"{document_id}_chunk{idx:03d}\"\n",
        "\n",
        "                output_data[chunk_key] = {\n",
        "                    \"chunk_id\": unique_chunk_id,\n",
        "                    \"chunk_text\": chunk[\"chunk_text\"],\n",
        "                    \"n_tokens\": chunk[\"n_tokens\"],\n",
        "                    \"item_code\": \"N/A\"\n",
        "                }\n",
        "\n",
        "            with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"خطا در {file_path.name}: {e}\")\n",
        "\n",
        "    print(\"پیش‌پردازش کامل شد.\")\n"
      ],
      "metadata": {
        "id": "xnGSjCT5lw93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import json\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# استخراج Lexicon مبتنی بر TF-IDF\n",
        "# --------------------------------------------------\n",
        "def extract_top_tfidf_by_item(\n",
        "    df,\n",
        "    text_col: str = \"chunk_text\",\n",
        "    item_col: str = \"item_code\",\n",
        "    topn: int = 50,\n",
        "    min_df: int = 2\n",
        ") -> Dict[str, List[Tuple[str, float]]]:\n",
        "\n",
        "    out = {}\n",
        "\n",
        "    for item in df[item_col].unique():\n",
        "        sub = df[df[item_col] == item]\n",
        "        texts = sub[text_col].fillna(\"\").tolist()\n",
        "\n",
        "        if len(texts) < 2:\n",
        "            out[item] = []\n",
        "            continue\n",
        "\n",
        "        vec = TfidfVectorizer(\n",
        "            ngram_range=(1, 3),\n",
        "            min_df=min_df,\n",
        "            max_features=5000\n",
        "        )\n",
        "\n",
        "        X = vec.fit_transform(texts)\n",
        "        scores = X.sum(axis=0).A1\n",
        "        terms = vec.get_feature_names_out()\n",
        "\n",
        "        ranked = sorted(\n",
        "            zip(terms, scores),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True\n",
        "        )[:topn]\n",
        "\n",
        "        out[item] = ranked\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# ذخیره / بارگذاری Lexicon\n",
        "# --------------------------------------------------\n",
        "def save_lexicons(lexicon_dict: dict, path):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(lexicon_dict, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "def load_lexicons(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# امتیازدهی Lexicon برای هر chunk\n",
        "# --------------------------------------------------\n",
        "def lexicon_score_for_chunk(\n",
        "    chunk_text: str,\n",
        "    lexicons: Dict[str, List[Tuple[str, float]]]\n",
        ") -> Dict[str, float]:\n",
        "\n",
        "    text = chunk_text.lower()\n",
        "    total_words = max(1, len(text.split()))\n",
        "    scores = {}\n",
        "\n",
        "    for cls, term_list in lexicons.items():\n",
        "        count = 0\n",
        "        for term, _ in term_list:\n",
        "            if re.search(rf\"\\b{re.escape(term.lower())}\\b\", text):\n",
        "                count += 1\n",
        "        scores[cls] = count / total_words\n",
        "\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "tzDtHn04lyzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embeddings.py\n",
        "\n",
        "import os\n",
        "import json\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from typing import List, Optional, Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "def _hash_text(text: str) -> str:\n",
        "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "\n",
        "class SBERTEmbedder:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"all-MiniLM-L6-v2\",\n",
        "        device: Optional[str] = None,\n",
        "        batch_size: int = 128,\n",
        "        cache_dir: Optional[str] = None,\n",
        "        use_fp16: bool = True\n",
        "    ):\n",
        "\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.batch_size = batch_size\n",
        "        self.cache_dir = cache_dir\n",
        "\n",
        "        self.model = SentenceTransformer(model_name, device=self.device)\n",
        "\n",
        "        if self.device == \"cuda\" and use_fp16:\n",
        "            self.model = self.model.half()\n",
        "\n",
        "        self.dim = self.model.get_sentence_embedding_dimension()\n",
        "\n",
        "        if cache_dir:\n",
        "            os.makedirs(cache_dir, exist_ok=True)\n",
        "            self.index_path = os.path.join(cache_dir, \"emb_index.json\")\n",
        "            self.index = self._load_index()\n",
        "        else:\n",
        "            self.index = None\n",
        "\n",
        "        print(f\"SBERT loaded on {self.device} | dim={self.dim}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Index handling\n",
        "    # ----------------------------\n",
        "    def _load_index(self):\n",
        "        if os.path.exists(self.index_path):\n",
        "            try:\n",
        "                with open(self.index_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    return json.load(f)\n",
        "            except Exception:\n",
        "                return {}\n",
        "        return {}\n",
        "\n",
        "    def _save_index(self):\n",
        "        if self.cache_dir:\n",
        "            with open(self.index_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(self.index, f)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Core embedding\n",
        "    # ----------------------------\n",
        "    def embed_texts(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        show_progress: bool = True\n",
        "    ) -> np.ndarray:\n",
        "\n",
        "        if not texts:\n",
        "            return np.zeros((0, self.dim), dtype=np.float32)\n",
        "\n",
        "        embeddings = self.model.encode(\n",
        "            texts,\n",
        "            batch_size=self.batch_size,\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True,\n",
        "            show_progress_bar=show_progress\n",
        "        )\n",
        "\n",
        "        return embeddings.astype(np.float32)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Cache-aware embedding\n",
        "    # ----------------------------\n",
        "    def embed_with_cache(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        ids: Optional[List[str]] = None\n",
        "    ) -> np.ndarray:\n",
        "\n",
        "        if not self.cache_dir or ids is None:\n",
        "            return self.embed_texts(texts)\n",
        "\n",
        "        results = [None] * len(texts)\n",
        "        to_compute = []\n",
        "\n",
        "        for i, (tid, text) in enumerate(zip(ids, texts)):\n",
        "            h = _hash_text(text)\n",
        "            fname = os.path.join(self.cache_dir, f\"{h}.npy\")\n",
        "\n",
        "            if os.path.exists(fname):\n",
        "                results[i] = np.load(fname)\n",
        "            else:\n",
        "                to_compute.append((i, h, text))\n",
        "\n",
        "        if to_compute:\n",
        "            new_embs = self.embed_texts([x[2] for x in to_compute])\n",
        "            for (i, h, _), emb in zip(to_compute, new_embs):\n",
        "                np.save(os.path.join(self.cache_dir, f\"{h}.npy\"), emb)\n",
        "                results[i] = emb\n",
        "\n",
        "        return np.vstack(results)\n"
      ],
      "metadata": {
        "id": "qHzzPZdHl0rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embeddings_mpnet.py\n",
        "\n",
        "import os\n",
        "import json\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "def _hash_text(text: str) -> str:\n",
        "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "\n",
        "class MPNetEmbedder:\n",
        "    \"\"\"\n",
        "    Embedder اختصاصی برای مدل all-mpnet-base-v2\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"all-mpnet-base-v2\",\n",
        "        device: Optional[str] = None,\n",
        "        batch_size: int = 32,\n",
        "        cache_dir: Optional[str] = None,\n",
        "        use_fp16: bool = True\n",
        "    ):\n",
        "\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.batch_size = batch_size\n",
        "        self.cache_dir = cache_dir\n",
        "\n",
        "        self.model = SentenceTransformer(model_name, device=self.device)\n",
        "\n",
        "        if self.device == \"cuda\" and use_fp16:\n",
        "            self.model = self.model.half()\n",
        "\n",
        "        self.dim = self.model.get_sentence_embedding_dimension()\n",
        "\n",
        "        if cache_dir:\n",
        "            os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "        print(\n",
        "            f\"MPNet loaded | device={self.device} | dim={self.dim} | batch={self.batch_size}\"\n",
        "        )\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # embedding اصلی\n",
        "    # -------------------------------------------------\n",
        "    def embed_texts(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        show_progress: bool = True\n",
        "    ) -> np.ndarray:\n",
        "\n",
        "        if not texts:\n",
        "            return np.zeros((0, self.dim), dtype=np.float32)\n",
        "\n",
        "        embeddings = self.model.encode(\n",
        "            texts,\n",
        "            batch_size=self.batch_size,\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True,\n",
        "            show_progress_bar=show_progress\n",
        "        )\n",
        "\n",
        "        return embeddings.astype(np.float32)\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # embedding با cache (بر اساس متن)\n",
        "    # -------------------------------------------------\n",
        "    def embed_with_cache(\n",
        "        self,\n",
        "        texts: List[str]\n",
        "    ) -> np.ndarray:\n",
        "\n",
        "        if not self.cache_dir:\n",
        "            return self.embed_texts(texts)\n",
        "\n",
        "        results = [None] * len(texts)\n",
        "        to_compute = []\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "            h = _hash_text(text)\n",
        "            path = os.path.join(self.cache_dir, f\"{h}.npy\")\n",
        "\n",
        "            if os.path.exists(path):\n",
        "                results[i] = np.load(path)\n",
        "            else:\n",
        "                to_compute.append((i, h, text))\n",
        "\n",
        "        if to_compute:\n",
        "            new_embs = self.embed_texts([x[2] for x in to_compute])\n",
        "            for (i, h, _), emb in zip(to_compute, new_embs):\n",
        "                np.save(os.path.join(self.cache_dir, f\"{h}.npy\"), emb)\n",
        "                results[i] = emb\n",
        "\n",
        "        return np.vstack(results)\n"
      ],
      "metadata": {
        "id": "dWCJJBxcl2Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# priors.py\n",
        "\n",
        "\n",
        "from typing import Dict, List\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "from typing import Iterator, Dict, Any\n",
        "\n",
        "try:\n",
        "    from .config import Config\n",
        "except ImportError:\n",
        "    class Config:\n",
        "        CLASSES = [\"E\", \"S\", \"G\" , \"EC\"]\n",
        "\n",
        "CLASSES = Config.CLASSES\n",
        "\n",
        "\n",
        "# fallback کاملاً امن\n",
        "UNIFORM_PRIOR = [1.0 / len(CLASSES)] * len(CLASSES)\n",
        "\n",
        "\n",
        "def load_priors(priors_path: Path) -> Dict[str, List[float]]:\n",
        "    \"\"\"\n",
        "    بارگذاری priors ساخته‌شده از لیبل‌ها (json)\n",
        "    \"\"\"\n",
        "    with open(priors_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        priors = json.load(f)\n",
        "\n",
        "    return priors\n",
        "\n",
        "\n",
        "def _list_to_dict(lst: List[float]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    [0.4,0.2,0.4] → {'E':0.4,'S':0.2,'G':0.4}\n",
        "    \"\"\"\n",
        "    return {\n",
        "        cls: float(lst[i]) if i < len(lst) else 0.0\n",
        "        for i, cls in enumerate(CLASSES)\n",
        "    }\n",
        "\n",
        "\n",
        "def normalize_prior(d: Dict[str, float]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    نرمال‌سازی برای اطمینان از sum=1\n",
        "    \"\"\"\n",
        "    s = sum(d.values())\n",
        "    if s <= 0:\n",
        "        return {k: 1.0 / len(d) for k in d}\n",
        "    return {k: v / s for k, v in d.items()}\n",
        "\n",
        "\n",
        "def get_prior_for_item(\n",
        "    item_code: str,\n",
        "    priors_map: Dict[str, List[float]],\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    گرفتن prior مناسب برای یک item_code\n",
        "    \"\"\"\n",
        "\n",
        "    key = (item_code or \"\").strip()\n",
        "    ku = key.upper()\n",
        "\n",
        "    # 1️⃣ exact\n",
        "    if ku in priors_map:\n",
        "        return normalize_prior(_list_to_dict(priors_map[ku]))\n",
        "\n",
        "    # 2️⃣ prefix (1A → 1)\n",
        "    if len(ku) >= 2 and ku[:2] in priors_map:\n",
        "        return normalize_prior(_list_to_dict(priors_map[ku[:2]]))\n",
        "\n",
        "    if len(ku) >= 1 and ku[:1] in priors_map:\n",
        "        return normalize_prior(_list_to_dict(priors_map[ku[:1]]))\n",
        "\n",
        "    # 3️⃣ unknown\n",
        "    return normalize_prior(\n",
        "        _list_to_dict(priors_map.get(\"unknown\", UNIFORM_PRIOR))\n",
        "    )\n"
      ],
      "metadata": {
        "id": "OTYYtC0wl5Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scoring.py\n",
        "\n",
        "import numpy as np\n",
        "from typing import Iterator, Dict, Any\n",
        "\n",
        "try:\n",
        "    from .config import Config\n",
        "except ImportError:\n",
        "    class Config:\n",
        "        CLASSES = [\"E\", \"S\", \"G\" , \"EC\"]\n",
        "\n",
        "CLASSES = Config.CLASSES\n",
        "\n",
        "\n",
        "def l2_norm(vec: np.ndarray) -> np.ndarray:\n",
        "    norm = np.linalg.norm(vec)\n",
        "    if norm == 0:\n",
        "        return vec\n",
        "    return vec / norm\n",
        "\n",
        "\n",
        "def compute_embedding_sims(\n",
        "    chunk_vec: np.ndarray,\n",
        "    class_reps: Dict[str, np.ndarray]\n",
        ") -> Dict[str, float]:\n",
        "    out = {}\n",
        "\n",
        "    if chunk_vec is None:\n",
        "        dim = next(iter(class_reps.values())).shape[0]\n",
        "        chunk_vec = np.zeros(dim)\n",
        "\n",
        "    chunk_n = l2_norm(chunk_vec.astype(float))\n",
        "\n",
        "    for cls in CLASSES:\n",
        "        rep = class_reps.get(cls)\n",
        "        if rep is None:\n",
        "            out[cls] = 0.0\n",
        "            continue\n",
        "\n",
        "        rep_n = l2_norm(rep.astype(float))\n",
        "        sim = float(np.dot(chunk_n, rep_n))\n",
        "        out[cls] = max(-1.0, min(1.0, sim))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def normalize_scores(d: Dict[str, float]) -> Dict[str, float]:\n",
        "    vals = np.array(list(d.values()), dtype=float)\n",
        "\n",
        "    if len(vals) == 0 or vals.max() - vals.min() <= 1e-9:\n",
        "        return {k: 1.0 / len(d) for k in d}\n",
        "\n",
        "    mn, mx = vals.min(), vals.max()\n",
        "    return {k: float((v - mn) / (mx - mn)) for k, v in d.items()}\n",
        "\n",
        "\n",
        "def softmax_dict(d: Dict[str, float], temp: float = 1.0) -> Dict[str, float]:\n",
        "    arr = np.array(list(d.values()), dtype=float) / float(temp)\n",
        "    arr -= arr.max()\n",
        "    ex = np.exp(arr)\n",
        "    probs = ex / ex.sum()\n",
        "    return {k: float(p) for k, p in zip(d.keys(), probs)}\n",
        "\n",
        "\n",
        "def combine_scores(\n",
        "    sim_scores: Dict[str, float],\n",
        "    lex_scores: Dict[str, float],\n",
        "    prior_scores: Dict[str, float],\n",
        "    alpha: float = 0.5,\n",
        "    gamma: float = 0.3,\n",
        "    beta: float = 0.2\n",
        "):\n",
        "    sim_n = normalize_scores({k: sim_scores.get(k, 0.0) for k in CLASSES})\n",
        "    lex_n = normalize_scores({k: lex_scores.get(k, 0.0) for k in CLASSES})\n",
        "\n",
        "    prior_scores = prior_scores or {}\n",
        "    prior_n = normalize_scores({\n",
        "        k: float(prior_scores.get(k, 1.0 / len(CLASSES)))\n",
        "        for k in CLASSES\n",
        "    })\n",
        "\n",
        "    total_w = alpha + gamma + beta\n",
        "    if total_w <= 0:\n",
        "        alpha, gamma, beta = 1.0, 0.0, 0.0\n",
        "        total_w = 1.0\n",
        "\n",
        "    alpha, gamma, beta = alpha / total_w, gamma / total_w, beta / total_w\n",
        "\n",
        "    combined = {\n",
        "        k: alpha * sim_n[k] + gamma * lex_n[k] + beta * prior_n[k]\n",
        "        for k in CLASSES\n",
        "    }\n",
        "\n",
        "    probs = softmax_dict(combined)\n",
        "    return combined, probs\n",
        "\n",
        "\n",
        "def decide_label(\n",
        "    probs: Dict[str, float],\n",
        "    thresh_label: float = 0.6,\n",
        "    thresh_multi: float = 0.45,\n",
        "    thresh_none: float = 0.4\n",
        "):\n",
        "    if not probs:\n",
        "        return \"ambiguous\", []\n",
        "\n",
        "    sorted_items = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
        "    max_label, max_prob = sorted_items[0]\n",
        "\n",
        "    if max_prob < thresh_none:\n",
        "        return \"ambiguous\", []\n",
        "\n",
        "    if max_prob >= thresh_label:\n",
        "        return \"single\", [max_label]\n",
        "\n",
        "    labels = [k for k, v in probs.items() if v >= thresh_multi]\n",
        "    if labels:\n",
        "        return \"multi\", labels\n",
        "\n",
        "    return \"single\", [max_label]\n"
      ],
      "metadata": {
        "id": "E2gTYb9-l787"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def build_class_representatives_from_seed(seed_lexicons, embedder):\n",
        "    reps = {}\n",
        "    for cls, terms in seed_lexicons.items():\n",
        "        if not terms:\n",
        "            reps[cls] = np.zeros(embedder.dim)\n",
        "            continue\n",
        "\n",
        "        embs = embedder.embed_texts(terms)  # GPU-safe\n",
        "        vec = embs.mean(axis=0)\n",
        "\n",
        "        norm = np.linalg.norm(vec)\n",
        "        reps[cls] = vec / (norm if norm > 0 else 1.0)\n",
        "\n",
        "    return reps\n",
        "\n",
        "\n",
        "def run_pipeline(\n",
        "    input_dir=INPUT_DIR,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    priors_map=None,\n",
        "    lexicon_path=None,\n",
        "):\n",
        "    if priors_map is None:\n",
        "        raise ValueError(\"priors_map must be provided\")\n",
        "\n",
        "    # ---------- load data ----------\n",
        "    df = load_cleaned_jsons(input_dir)\n",
        "    print(f\"Loaded {len(df)} chunks\")\n",
        "\n",
        "    df[\"chunk_text_clean\"] = (\n",
        "        df[\"chunk_text\"].fillna(\"\").apply(clean_chunk_text)\n",
        "    )\n",
        "\n",
        "    # ---------- lexicons ----------\n",
        "    if lexicon_path:\n",
        "        seed_lexicons = load_lexicons(lexicon_path)\n",
        "    else:\n",
        "        seed_lexicons = {\n",
        "            \"E\": E + core_esg_seeds,\n",
        "            \"S\": S + core_esg_seeds,\n",
        "            \"G\": G + core_esg_seeds,\n",
        "        }\n",
        "\n",
        "    # ---------- embedder (GPU inside) ----------\n",
        "    embedder = SBERTEmbedder(\n",
        "        model_name=EMBEDDING_MODEL,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        cache_dir=\"emb_cache\",\n",
        "    )\n",
        "\n",
        "    class_reps = build_class_representatives_from_seed(\n",
        "        seed_lexicons, embedder\n",
        "    )\n",
        "\n",
        "    # ---------- embeddings ----------\n",
        "    ids = df[\"chunk_id\"].tolist()\n",
        "    texts = df[\"chunk_text_clean\"].tolist()\n",
        "\n",
        "    print(\"Computing embeddings (GPU if available)...\")\n",
        "    all_embs = embedder.embed_with_cache(\n",
        "        ids, texts, show_progress=True\n",
        "    )\n",
        "\n",
        "    # ---------- scoring ----------\n",
        "    results = []\n",
        "    n = len(df)\n",
        "\n",
        "    for i in range(n):\n",
        "        row = df.iloc[i]\n",
        "        emb = all_embs[i]\n",
        "\n",
        "        sim_scores = compute_embedding_sims(emb, class_reps)\n",
        "        lex_scores = lexicon_score_for_chunk(\n",
        "            row[\"chunk_text_clean\"], seed_lexicons\n",
        "        )\n",
        "\n",
        "        prior_dict = get_prior_for_item(\n",
        "            row.get(\"item_code\", \"unknown\"),\n",
        "            priors_map,\n",
        "        )\n",
        "\n",
        "        _, probs = combine_scores(\n",
        "            sim_scores,\n",
        "            lex_scores,\n",
        "            prior_dict,\n",
        "            alpha=ALPHA,\n",
        "            beta=BETA,\n",
        "            gamma=GAMMA,\n",
        "        )\n",
        "\n",
        "        label_type, labels = decide_label(\n",
        "            probs,\n",
        "            thresh_label=THRESH_LABEL,\n",
        "            thresh_multi=THRESH_MULTI,\n",
        "            thresh_none=THRESH_NONE,\n",
        "        )\n",
        "\n",
        "        results.append(\n",
        "            {\n",
        "                \"file_id\": row[\"file_id\"],\n",
        "                \"item_code\": row[\"item_code\"],\n",
        "                \"chunk_id\": row[\"chunk_id\"],\n",
        "                \"label_type\": label_type,\n",
        "                \"labels\": labels,\n",
        "                \"probs\": probs,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    outdf = pd.DataFrame(results)\n",
        "    output_path = output_dir / \"chunks_esg_labels.csv\"\n",
        "    outdf.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"Saved results to {output_path}\")\n",
        "    return outdf\n"
      ],
      "metadata": {
        "id": "4tx6Uihxl8aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate.py\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "def sample_for_manual_review(labels_csv, n=200, stratify_col=\"item_code\"):\n",
        "    df = pd.read_csv(labels_csv)\n",
        "    # stratified sample by item_code (simple)\n",
        "    sample = df.groupby(stratify_col, group_keys=False).apply(lambda x: x.sample(max(1, int(n * len(x)/len(df))))).reset_index(drop=True)\n",
        "    sample = sample.sample(min(n, len(df)), random_state=42)\n",
        "    # export sample for annotation\n",
        "    sample.to_csv(labels_csv.replace(\".csv\", f\".sample_{n}.csv\"), index=False)\n",
        "    print(\"Sample saved for manual review.\")\n",
        "    return sample\n"
      ],
      "metadata": {
        "id": "F5gRjPJ4l-Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from embeddings import SBERTEmbedder\n",
        "# from pipeline import build_class_representatives_from_seed, run_pipeline\n",
        "# from config import EMBEDDING_MODEL, BATCH_SIZE, CLASSES\n",
        "\n",
        "embedder = SBERTEmbedder(model_name=EMBEDDING_MODEL, device=\"cpu\", batch_size=BATCH_SIZE, cache_dir=\"emb_cache_test\")\n",
        "# seed = {\"E\":[\"emission\",\"carbon\"], \"S\":[\"employee\",\"safety\"], \"G\":[\"board\",\"audit\"]}\n",
        "\n",
        "core_esg_seeds = [\"Sustainable\", \"Sustainability\", \"ESG\", \"Development\", \"Sustainable development\",\n",
        "                  \"SDGs\", \"Environmental, social, and governance\", \"Sustainable finance\",\n",
        "                  \"Sustainable innovation\", \"Economic sustainability\", \"Economy\", \"Green economy\",\n",
        "                  \"Crisis\", \"Future Needs\"]\n",
        "\n",
        "E = [\"emission\", \"carbon\", \"Environmental\", \"Environmental protection\", \"Environmental impact\",\n",
        "     \"environment\", \"climate\", \"Climate mitigation\", \"Climate change\", \"Carbon footprint\",\n",
        "     \"Carbon emissions\", \"Pollutants\", \"Greenhouse Gas Emissions\", \"Decarbonization\",\n",
        "     \"Renewable energy\", \"Clean energy\", \"Energy efficiency\", \"Recycling\", \"Circular economy\",\n",
        "     \"Waste management\", \"Zero waste\", \"Natural Resources\", \"Resource management\",\n",
        "     \"Earth\", \"Air\", \"biodiversity\", \"Atmospheric\", \"Water\", \"pollution\",  \"Green technology\",\n",
        "     \"Pollution reduction\", \"Drought\", \"Water conservation\", \"Ground Warming\",\n",
        "     \"Global Warming\", \"Species extinction\", \"Ecosystem preservation\", \"Sustainable materials\",\n",
        "     \"Life cycle assessment\", \"Eco-friendly\", \"Sustainable agriculture\"]\n",
        "\n",
        "S = [\"employee\", \"safety\", \"Human rights\", \"CSR\", \"Corporate\", \"social\", \"responsibility\", \"Society\",\n",
        "     \"Responsible consumption\", \"Demographic changes\", \"Famine\", \"Better life\", \"Diversity\", \"Inclusion\",\n",
        "     \"Equality\", \"Labor practices\", \"Worker health\", \"Sustainable supply chain\"]\n",
        "\n",
        "G = [\"Economic\", \"Economy\", \"Green economy\", \"governance\", \"Risk management\", \"Executive compensation\",\n",
        "     \"Shareholder\", \"audit\", \"board\", \"Management structure\", \"Fiduciary duty\", \"Internal controls\",\n",
        "     \"Ethics\", \"Compliance\", \"Regulation\", \"Anti-corruption\", \"Corruption\", \"Bribery\", \"Legal\",\n",
        "     \"Code of conduct\", \"Disclosure\", \"Transparency\", \"Non-Financial\", \"Reporting\", \"Accountability\",\n",
        "     \"Data security\"]\n",
        "\n",
        "seed = { \"E\": E + core_esg_seeds,\n",
        "         \"S\": S + core_esg_seeds,\n",
        "         \"G\": G + core_esg_seeds\n",
        "       }\n",
        "\n",
        "\n",
        "reps = build_class_representatives_from_seed(seed, embedder)\n",
        "print(\"class reps dim:\", {k:v.shape for k,v in reps.items()})\n",
        "# سپس run pipeline روی یک پوشه نمونه کوچک\n",
        "# out = run_pipeline(input_dir=\"path/to/small_sample\", output_dir=Path(\"outs_test\"), extract_seeds=False, lexicon_path=None)\n",
        "out = run_pipeline(INPUT_DIR, OUTPUT_DIR , extract_seeds=False, lexicon_path=None)\n",
        "\n",
        "print(out.head())"
      ],
      "metadata": {
        "id": "wm-6YnIhnLH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mhpzhSkKnQI3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}